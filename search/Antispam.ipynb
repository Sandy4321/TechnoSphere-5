{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import base64\n",
    "import csv\n",
    "import gzip\n",
    "import zlib\n",
    "import nltk\n",
    "from collections import namedtuple\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import linear_model\n",
    "from  sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]'] or isinstance(element, Comment):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    \n",
    "    links = []\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "        links.append(link.get('href'))\n",
    "    \n",
    "    return ' '.join(visible_texts), links\n",
    "\n",
    "def tokenize_me(file_text):\n",
    "    tokens = nltk.word_tokenize(file_text)\n",
    "    tokens = [i.lower() for i in tokens if ( i not in string.punctuation )]\n",
    " \n",
    "    stop_words = stopwords.words('russian')\n",
    "    stop_words.extend(map(lambda x: x.decode('utf8'), \n",
    "                          ['что', 'это', 'так', 'вот', 'быть', 'как', 'в', '—', 'к', 'на']))\n",
    "    tokens = [i for i in tokens if ( i not in stop_words )]     \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocItem = namedtuple('DocItem', ['doc_id', 'is_spam', 'url', 'html_text'])\n",
    "\n",
    "def load_csv(input_file_name):    \n",
    "    with gzip.open(input_file_name) if input_file_name.endswith('gz') else open(input_file_name)  as input_file:\n",
    "        headers = input_file.readline()\n",
    "        for i, line in enumerate(input_file):\n",
    "            parts = line.strip().split('\\t')\n",
    "            url_id = int(parts[0])                                        \n",
    "            mark = int(parts[1])                    \n",
    "            url = parts[2]\n",
    "            pageInb64 = parts[3]\n",
    "            html_data = base64.b64decode(pageInb64).decode('utf8')            \n",
    "            yield DocItem(url_id, mark, url, html_data)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_from_file:\n",
    "    texts, marks = [], []\n",
    "    for item in tqdm(load_csv('./antispam-infopoisk/kaggle_train_data_tab.csv.gz')):\n",
    "        text, links = text_from_html(item.html_text)\n",
    "        texts.append(tokenize_me(text) + ' '.join(links))\n",
    "        marks.append(item.is_spam)\n",
    "    pickle.dump([texts, marks], open('train.p', 'w'))\n",
    "\n",
    "\n",
    "    test_text, Id = [], []\n",
    "    for item in tqdm(load_csv('./antispam-infopoisk/kaggle_test_data_tab.csv.gz')):\n",
    "        text, links = text_from_html(item.html_text)\n",
    "        test_text.append(tokenize_me(text) + ' ' + ' '.join(links))\n",
    "        Id.append(item.doc_id)\n",
    "    pickle.dump((test_text, Id), open('test.p', 'w'))\n",
    "\n",
    "\n",
    "else:\n",
    "    texts, marks = pickle.load(open('train.p')) \n",
    "    test_text, Id = pickle.load(open('test.p', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(texts, marks, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_text = Pipeline([('tfidf', TfidfVectorizer() ),\n",
    "                     ('clf', SGDClassifier(verbose=False))])\n",
    "#sgd_text = clf_text.fit(X_train, y_train)\n",
    "\n",
    "#y_pred = sgd_text.predict(X_test)\n",
    "#print f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df_range = np.linspace(0.001, 0.1, 5).round(3)\n",
    "max_df_range = np.linspace(0.6, 1, 5).round(2)\n",
    "ngram_range = [(1, 3)]\n",
    "\n",
    "\n",
    "parameters = {'tfidf__ngram_range': ngram_range,\n",
    "              'tfidf__min_df' : 0.0,\n",
    "              'tfidf__max_df' : 1.0,\n",
    "              'tfidf__use_idf': [True],\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(sgd_text, parameters, n_jobs=-1, verbose=True, scoring='f1', cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidf__max_df': 1.0,\n",
       " 'tfidf__min_df': 0.001,\n",
       " 'tfidf__ngram_range': (1, 2),\n",
       " 'tfidf__use_idf': True}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9679124197810494\n"
     ]
    }
   ],
   "source": [
    "y_pred = gs_clf.predict(X_test)\n",
    "print f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_text = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', LogisticRegression(verbose=False))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1,3))),\n",
    "                     ('clf', LogisticRegression(verbose=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), norm=u'l2', preprocessor=None, smooth_idf=Tru...l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=False, warm_start=False))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df_range = np.linspace(0.001, 0.1, 5).round(3)\n",
    "max_df_range = np.linspace(0.6, 1, 5).round(2)\n",
    "ngram_range = [(1, 2)]\n",
    "\n",
    "\n",
    "parameters = {'tfidf__ngram_range': ngram_range,\n",
    "              'tfidf__min_df' : min_df_range,\n",
    "              'tfidf__max_df' : max_df_range,\n",
    "              'tfidf__use_idf': [True],\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = Pipeline([('tfidf', TfidfVectorizer(max_df=1.0, min_df=0.001, use_idf=True, ngram_range=(1,3)) ),\n",
    "                     ('clf', SGDClassifier(verbose=False))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e.zholkovskiy/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=0.001,\n",
       "        ngram_range=(1, 3), norm=u'l2', preprocessor=None, smooth_idf...', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=False, warm_start=False))])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9763387297633872"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                ('clf', linear_model.RandomizedLogisticRegression())])\n",
    "clf = clf.fit(texts, marks)\n",
    "prediction = clf.predict(texts)\n",
    "print np.mean(prediction == marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predict(model, test_text, Id):\n",
    "    prediction = model.predict(test_text)\n",
    "    with open('my_submission.csv' , 'wb') as fout:\n",
    "        writer = csv.writer(fout)\n",
    "        writer.writerow(['Id','Prediction'])\n",
    "        for i, item in enumerate(prediction):\n",
    "            writer.writerow([Id[i], item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_predict(lr_clf, test_text, Id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
